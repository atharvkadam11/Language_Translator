{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy keras tensorflow pyttsx3 gtts pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Language Translation using Neural Networks \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading and preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Sanskrit.txt', 'r+', encoding=\"utf8\")\n",
    "x = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('English.txt', 'r+', encoding=\"utf8\")\n",
    "y = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]= x[0].strip('\\ufeffMMA')\n",
    "y[0]= y[0].strip('\\ufeffMMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "for i in range(0,len(x)):\n",
    "    x[i] = x[i].strip('\\n')\n",
    "    x[i] = ''.join(ch for ch in x[i] if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(y)):\n",
    "    y[i] = y[i].lower()\n",
    "    y[i] = y[i].strip('\\n')\n",
    "    y[i] = ''.join(ch for ch in y[i] if ch not in exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanskrit Text: सञ्जय उवाच ।दृष्ट्वा तु पाण्डवानीकं व्यूढं दुर्योधनस्तदा ।आचार्यमुपसङ्गम्य राजा वचनमब्रवीत् ।। ।। \n",
      "\n",
      "English Text: sanjay said on observing the pandava army standing in military formation king duryodhan approached his teacher dronacharya and said the following words\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanskrit Text:\",x[1],\"\\n\")\n",
    "print(\"English Text:\",y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = []\n",
    "for i in range(0,len(y)):\n",
    "    english_words.append(y[i].split())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = [j for sub in english_words for j in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique English words: 2861\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Unique English words:\",len(set(english_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sans_words = []\n",
    "for i in range(0,len(x)):\n",
    "    sans_words.append(x[i].split())  \n",
    "sans_words = [j for sub in sans_words for j in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique sans words: 4047\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Unique sans words:\",len(set(sans_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sansvocab = len(set(sans_words))\n",
    "engvocab = len(set(english_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like there are more number of unique words in sans.....as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_sans=[]\n",
    "for i in range(0,len(x)):\n",
    "    length_sans.append(len(x[i].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_english=[]\n",
    "for i in range(0,len(y)):\n",
    "    length_english.append(len(y[i].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of words in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.20142857142857"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(length_english)/len(length_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.938571428571429"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(length_sans)/len(length_sans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(max(length_english))\n",
    "print(max(length_sans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "english_words_counter = collections.Counter([word for sentence in y for word in sentence.split()])\n",
    "sans_words_counter = collections.Counter([word for sentence in x for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1879),\n",
       " ('of', 1141),\n",
       " ('and', 978),\n",
       " ('to', 555),\n",
       " ('in', 528),\n",
       " ('is', 412),\n",
       " ('are', 348),\n",
       " ('i', 335),\n",
       " ('who', 290),\n",
       " ('me', 249)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('च', 229),\n",
       " ('न', 173),\n",
       " ('स', 73),\n",
       " ('मे', 61),\n",
       " ('मां', 48),\n",
       " ('ते', 41),\n",
       " ('हि', 39),\n",
       " ('उवाच', 36),\n",
       " ('पार्थ', 36),\n",
       " ('कर्म', 35)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sans_words_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x) \n",
    "    return tokenizer.texts_to_sequences(x), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer:\n",
    "\n",
    "Now that our corpus is ready we have to represent it in a way that the neural network can understand, So we convert the text representation to number representation. In words based representation each word his assigned a number abd in character based representation each character is assigned a number. I am using a word level model for its simpler complexity\n",
    "\n",
    "Keras Tokenizer simplifies the representation process for us (This class allows to vectorize a text corpus, by turning each text into either a sequence of integers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'धृतराष्ट्र उवाच धर्मक्षेत्रे कुरुक्षेत्रे समवेता युयुत्सवः मामकाः पाण्डवाश्चैव किमकुर्वत सञ्जय '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65, 8, 1165, 14, 1166, 1167, 1168, 1169, 200, 1170, 110, 110]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=(tokenize(x))\n",
    "z[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8, 'by': 9, 'jove': 10, 'my': 11, 'study': 12, 'of': 13, 'lexicography': 14, 'won': 15, 'a': 16, 'prize': 17}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [9, 10, 11, 2, 12, 13, 14, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',]\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding:\n",
    "\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def pad(x, length=None):\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT IS ALWAYS A LENGTH 10 ARRAY....FILLED BY 0s IN THE END\n",
      "Sequence 1 in x\n",
      "  Input:  [1 2 3 4 5 6 1 7 8]\n",
      "  Output: [1 2 3 4 5 6 1 7 8 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [ 9 10 11  2 12 13 14 15 16 17]\n",
      "  Output: [ 9 10 11  2 12 13 14 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "test_pad = pad(text_tokenized)\n",
    "print(\"OUTPUT IS ALWAYS A LENGTH 10 ARRAY....FILLED BY 0s IN THE END\")\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply all the tested preprocessing functions to our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    print('shape before: ', preprocess_y.shape)\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    print('shape after: ', preprocess_y.shape)\n",
    "        \n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 40)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_x, x_tk = tokenize(x)\n",
    "preprocess_x = pad(preprocess_x)\n",
    "preprocess_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before:  (700, 139)\n",
      "shape after:  (700, 139, 1)\n"
     ]
    }
   ],
   "source": [
    "preproc_sans_sentences, preproc_english_sentences, sans_tokenizer, english_tokenizer =\\\n",
    "    preprocess(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning a number to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('च', 1), ('न', 2), ('स', 3), ('मे', 4), ('मां', 5)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sans_tokenizer.word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1), ('of', 2), ('and', 3), ('to', 4), ('in', 5)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(english_tokenizer.word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logits to text\n",
    "\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want. We want the sans translation. The function logits_to_text will bridge the gab between the logits from the neural network to the sans translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tok enizer):\n",
    "\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sans_sentences shape:  (700, 139, 1)\n",
      "english_sentences  shape:  (700, 40)\n",
      "output sequence length:  139\n"
     ]
    }
   ],
   "source": [
    "print(\"sans_sentences shape: \", preproc_english_sentences.shape)\n",
    "print(\"english_sentences  shape: \", preproc_sans_sentences.shape)\n",
    "print('output sequence length: ', preproc_english_sentences.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = pad(preproc_sans_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 139, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Embedding Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, sans_vocab_size, english_vocab_size, learning_rate=0.1):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max(sans_vocab_size, english_vocab_size) ,128 , input_length=output_sequence_length))\n",
    "    model.add(LSTM(128, dropout=0.1, return_sequences=True))\n",
    "    model.add(Dense(english_vocab_size, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the input for processing embeddings\n",
    "tmp_x = pad(preproc_sans_sentences, preproc_english_sentences.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 139)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 139, 128)          518144    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 139, 128)          131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 139, 2862)         369198    \n",
      "=================================================================\n",
      "Total params: 1,018,926\n",
      "Trainable params: 1,018,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "6/6 [==============================] - 4s 387ms/step - loss: 4.4820 - accuracy: 0.6109 - val_loss: 3.5819 - val_accuracy: 0.7573\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 2s 347ms/step - loss: 3.2207 - accuracy: 0.7446 - val_loss: 3.2962 - val_accuracy: 0.7573\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 2.7499 - accuracy: 0.7447 - val_loss: 2.9933 - val_accuracy: 0.7571\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 2.4236 - accuracy: 0.7436 - val_loss: 2.6906 - val_accuracy: 0.7576\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 3s 478ms/step - loss: 2.8107 - accuracy: 0.7318 - val_loss: 2.7353 - val_accuracy: 0.7586\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 3s 465ms/step - loss: 2.3819 - accuracy: 0.7486 - val_loss: 2.4576 - val_accuracy: 0.7604\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 3s 440ms/step - loss: 2.1438 - accuracy: 0.7491 - val_loss: 2.3751 - val_accuracy: 0.7611\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 3s 436ms/step - loss: 2.0792 - accuracy: 0.7501 - val_loss: 2.2831 - val_accuracy: 0.7611\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 3s 452ms/step - loss: 2.0405 - accuracy: 0.7515 - val_loss: 2.3922 - val_accuracy: 0.7607\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 3s 440ms/step - loss: 2.3082 - accuracy: 0.7524 - val_loss: 2.6001 - val_accuracy: 0.7616\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 3s 471ms/step - loss: 2.0530 - accuracy: 0.7543 - val_loss: 2.3407 - val_accuracy: 0.7606\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 3s 470ms/step - loss: 1.9564 - accuracy: 0.7561 - val_loss: 2.2866 - val_accuracy: 0.7618\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 3s 443ms/step - loss: 1.8702 - accuracy: 0.7584 - val_loss: 2.5830 - val_accuracy: 0.7618\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 3s 444ms/step - loss: 1.9445 - accuracy: 0.7601 - val_loss: 2.4019 - val_accuracy: 0.7615\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 3s 449ms/step - loss: 1.8508 - accuracy: 0.7618 - val_loss: 2.3798 - val_accuracy: 0.7618\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 3s 445ms/step - loss: 1.8541 - accuracy: 0.7629 - val_loss: 2.3479 - val_accuracy: 0.7616\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 3s 490ms/step - loss: 1.7742 - accuracy: 0.7651 - val_loss: 2.6354 - val_accuracy: 0.7612\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 3s 483ms/step - loss: 1.9027 - accuracy: 0.7663 - val_loss: 2.3806 - val_accuracy: 0.7617\n",
      "Epoch 19/20\n",
      "6/6 [==============================] - 3s 449ms/step - loss: 1.9774 - accuracy: 0.7664 - val_loss: 2.5037 - val_accuracy: 0.7620\n",
      "Epoch 20/20\n",
      "6/6 [==============================] - 3s 437ms/step - loss: 1.8816 - accuracy: 0.7676 - val_loss: 2.4064 - val_accuracy: 0.7608\n"
     ]
    }
   ],
   "source": [
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    len(sans_tokenizer.word_index)+1,\n",
    "    len(english_tokenizer.word_index)+1)\n",
    "if os.path.exists(os.path.join(\"model\", \"Final_LSTM.h5\"))== False:\n",
    "    embedrnn = embed_rnn_model.fit(tmp_x, preproc_english_sentences, batch_size=100, epochs=20, validation_split=0.2)\n",
    "else:\n",
    "    embed_rnn_model = load_model(os.path.join(\"model\", \"Final_LSTM.h5\"))\n",
    "\n",
    "# embedrnn = embed_rnn_model.fit(tmp_x, preproc_english_sentences, batch_size=50, epochs=50, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_rnn_model.save(os.path.join(\"model\", \"Final_LSTM.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation loss seems to converge much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accurancy:  0.7694553136825562\n"
     ]
    }
   ],
   "source": [
    "score = embed_rnn_model.evaluate(tmp_x, preproc_english_sentences, verbose=0)\n",
    "print(\"Train accurancy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'यदा यदा हि धर्मस्य ग्लानिर्भवति भारत अभ्युत्थानमधर्मस्य तदात्मानं सृजाम्यहम्  '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[167][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "those those is the the the the the the all <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:])[167][:], english_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitting the translated text until a word repeats itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limiten(n):\n",
    "    inp = logits_to_text(embed_rnn_model.predict(tmp_x[:])[n][:], english_tokenizer)\n",
    "    inp = inp.split(\" \")\n",
    "\n",
    "    nl = []\n",
    "\n",
    "    try:\n",
    "        for i in range(len(inp)+1):\n",
    "            if inp[i] != inp[i+1] and inp[i] != inp[i+2]:\n",
    "                # z = inp[i].join(inp[i])\n",
    "                nl.append(inp[i])\n",
    "                # print(str(nl[-1]))\n",
    "            a = ' '.join(nl)\n",
    "    except:\n",
    "        IndexError\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "those is the all\n"
     ]
    }
   ],
   "source": [
    "p = limiten(167)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "def enTTS(n):\n",
    "    engine = pyttsx3.init()\n",
    "    voices = engine.getProperty('voices')\n",
    "    engine.setProperty('voice', voices[1].id)\n",
    "\n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', 150)\n",
    "\n",
    "    engine.say(\"English\\n\")\n",
    "    engine.say(limiten(n))\n",
    "\n",
    "    engine.save_to_file(limiten(n), \"en_x[{}].mp3\".format(n))\n",
    "    engine.runAndWait()\n",
    "\n",
    "    return print(\"File saved as en_x[{}].mp3\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as en_x[167].mp3\n"
     ]
    }
   ],
   "source": [
    "enTTS(167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanskrit Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.6.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "from pygame import mixer\n",
    "import time\n",
    "\n",
    "def saTTS(n):\n",
    "    txt = x[n]\n",
    "\n",
    "    obj = gTTS(text=txt, lang='hi', slow=False, )\n",
    "\n",
    "    obj.save(\"sa_x[{}].mp3\".format(n))    \n",
    "\n",
    "    mixer.init()\n",
    "    mixer.music.load(\"sa_x[{}].mp3\".format(n))\n",
    "    mixer.music.play()\n",
    "    \n",
    "    while mixer.music.get_busy():\n",
    "        time.sleep(1)\n",
    "\n",
    "    return print(\"File saved as sa_x[{}].mp3\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as sa_x[167].mp3\n"
     ]
    }
   ],
   "source": [
    "saTTS(167)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
